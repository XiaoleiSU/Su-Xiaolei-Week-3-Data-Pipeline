# Data Cleaning and Validation Pipeline

A Python pipeline for cleaning and validating scraped article data. Designed for raw web-scraped content with various data quality issues.

## File Structure

```
Assignment 1/
├── cleaner.py           # Cleaning utilities (HTML, whitespace, encoding, dates)
├── validator.py         # Validation utilities (required fields, URL, length)
├── sample_data.json     # Sample input data (12 raw articles with quality issues)
├── cleaned_output.json  # Output: cleaned records (generated by cleaner.py)
├── quality_report.txt   # Output: validation report (generated by validator.py)
├── run_pipeline.py      # Optional: full pipeline (clean + validate)
├── README.md
└── prompt-log.md
```

## Features
- **Cleaning**: Remove HTML, normalize whitespace/encoding, standardize dates
- **Validation**: Check required fields, validate URLs, enforce length requirements  
- **Reporting**: Generate quality metrics and validation statistics

## How to Run

**Run modules independently:**
```bash
python cleaner.py      # Reads sample_data.json → writes cleaned_output.json
python validator.py    # Reads sample_data.json → writes quality_report.txt
```

**Run full pipeline (optional):**
```bash
python run_pipeline.py # Loads, cleans, validates; outputs both files + validation metadata
```

Requires Python 3.7+. No external dependencies (stdlib only).
