[README.md](https://github.com/user-attachments/files/25118892/README.md)
# Data Cleaning and Validation Pipeline

A Python pipeline for cleaning and validating scraped article data. Designed for raw web-scraped content with various data quality issues.

## File Structure

```
Assignment 1/
├── cleaner.py           # Cleaning utilities (HTML, whitespace, encoding, dates)
├── validator.py         # Validation utilities (required fields, URL, length)
├── sample_data.json     # Sample input data (12 raw articles with quality issues)
├── cleaned_output.json  # Output: cleaned records (generated by cleaner.py)
├── quality_report.txt   # Output: validation report (generated by validator.py)
├── run_pipeline.py      # Optional: full pipeline (clean + validate)
├── README.md
└── prompt-log.md
```

## Features

**cleaner.py**
- Remove HTML tags and entities (`&nbsp;`, `&amp;`, `&#8220;`, etc.)
- Normalize whitespace (collapse multiple spaces, trim)
- Normalize Unicode encoding (NFC, BOM, zero-width chars)
- Standardize dates to ISO format (YYYY-MM-DD) from US, European, and text formats
- Handle special characters (curly quotes, en/em dashes)

**validator.py**
- Check required fields: title, content, url
- Validate URL format (http/https)
- Enforce minimum content and title lengths
- Flag invalid records with specific reasons
- Generate quality report: total/valid/invalid counts, field completeness %, common failures

## How to Run

**Run modules independently:**
```bash
python cleaner.py      # Reads sample_data.json → writes cleaned_output.json
python validator.py    # Reads sample_data.json → writes quality_report.txt
```

**Run full pipeline (optional):**
```bash
python run_pipeline.py # Loads, cleans, validates; outputs both files + validation metadata
```

Requires Python 3.7+. No external dependencies (stdlib only).
